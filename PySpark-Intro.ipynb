{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a spark session\n",
    "spark = SparkSession.builder.appName('Demo').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.50.66.117:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109135390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[98] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(100000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = sc.parallelize([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[94] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = sc.parallelize(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.map(lambda x: x**2).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    for i in range(2,int(n**0.5)+1):\n",
    "        if n%i==0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[108] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.filter(lambda x: is_prime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_squares = r2.filter(is_prime).map(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[112] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.filter(is_prime).map(lambda x: x**2).fiter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([[1, 2], [4, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1, 2, 3]).mapValues(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1, 2, 3]).flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark's basic abstraction is an **RDD**.\n",
    " 1. RDDs are fault-tolerant, parallel data structures\n",
    " 2. Allow coarse-grained transformations in contrast to array or database => logging transformations (lineage) yields fault tolerance\n",
    " 3. RDD is a read-only, partitioned collection of records (immutable)\n",
    "\n",
    "#### Two kinds of operations:\n",
    "1. **Transformations** are lazy: _map_, _filter_, and _join_ => (can combine multiple mappers)\n",
    "2. **Actions** are eager: _count_, _collect_, _save_, _take_, _saveAsTextFile_\n",
    "\n",
    "#### Representation of RDDs / The RDD Interface\n",
    " - Set of _partitions_\n",
    " - Dependencies on parent RDDs\n",
    " - Function to compute a partition given parents\n",
    "\n",
    "#### Details\n",
    " * **Narrow** and **wide** transformations: **Wide** transformation requires shuffling across partitions.\n",
    " * RDDs are cacheable\n",
    " * Spark job is a **DAG of RDDs**. (Hence no need of intermediate data materialization)\n",
    " * Task is data + computation. (1 partition, 1 thread)\n",
    " * When you create the SparkContext, each worker starts an executor. This is a separate process (JVM), and it loads your jar too. The executors connect back to your driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rdd_lineage.png](assets/rdd_lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### [Official Docs](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tales = sc.textFile('/Users/300041370/data/shakespeare_tales.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Project Gutenberg's Tales from Shakespeare, by Charles Lamb and Mary  Lamb\",\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '',\n",
       " 'Title: Tales from Shakespeare',\n",
       " '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tales.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 2)], [(2, 3)]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([{1:2}, {2:3}]).map(lambda x: list(x.items())).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Project', \"Gutenberg's\", 'Tales'], [''], ['This', 'eBook', 'is']]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tales.map(lambda line: line.split(' ')[:3]).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a function `func` undergoes **map** transformation `RDD[str]` to `RDD[List[str]]`, then\n",
    "**flatMap** transformation takes `RDD[str]` to `RDD[str]`. It basically _flattens_ the mapped RDD\n",
    "\n",
    "**map**: 1 to 1\n",
    "\n",
    "**flatMap**: 1 to many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(ls):\n",
    "\treturn [item for sublist in ls for item in sublist]\n",
    "\n",
    "flatten([[1, 2], [3, 4], [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/300041370/data/shakespeare_tales.txt MapPartitionsRDD[118] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " \"Gutenberg's\",\n",
       " 'Tales',\n",
       " 'from',\n",
       " 'Shakespeare,',\n",
       " 'by',\n",
       " 'Charles',\n",
       " 'Lamb',\n",
       " 'and',\n",
       " 'Mary']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tales.flatMap(lambda line: line.split(' ')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tales.flatMap(lambda line: line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: `countByValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More general method\n",
    "**`groupByKey`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 78), ('Tales', 7), ('Lamb', 6)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word, 1)).groupByKey().map(lambda x: (x[0], len(x[1]))).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 78), ('Tales', 7), ('Lamb', 6), ('Mary', 3), ('', 2609)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word, 1)).groupByKey().mapValues(len).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better & General method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = words.map(lambda x: (x, 1)).reduceByKey(lambda c1, c2: c1+c2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4960),\n",
       " ('and', 3774),\n",
       " ('to', 3577),\n",
       " ('of', 2983),\n",
       " ('', 2609),\n",
       " ('he', 1921),\n",
       " ('a', 1875),\n",
       " ('his', 1833),\n",
       " ('in', 1476),\n",
       " ('was', 1450)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(counts, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda c1, c2: c1+c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4960), ('and', 3774), ('to', 3577), ('of', 2983), ('', 2609)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.sortBy(lambda x: x[1], False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey` works like MapReduce's **_combiner_** (also called semi-reducer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![groupbykey.jpg](assets/groupbykey.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reducebykey.jpg](assets/reducebykey.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache\n",
    "\n",
    "If multiple actions are called on an RDD, it will be executed repeatedly. Hence, there is option to persist the RDD's value - in memory and disk. Caching follows LRU policy to evict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    for i in range(2,int(n**0.5)+1):\n",
    "        if n%i==0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural = sc.parallelize(range(int(1e6)))\n",
    "primes = natural.filter(is_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes.cache() # keep primes in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes.unpersist() # flush primes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialized tool beats a general purpose tool\n",
    "Specialized tools usually outperform or are more accurate than general purpose tools ([Hettinger](https://twitter.com/raymondh/status/974018651308179456))\n",
    " - `math.sqrt(x)` is more accurate than `x ** 0.5`\n",
    " - `math.log2()` is exact for powers of two\n",
    "    ```python\n",
    "    from math import log, log2\n",
    "    all(log(2 ** x, 2) == x for x in range(100)) # False\n",
    "    all(log2(2 ** x) == x for x in range(100)) # True\n",
    "    ```\n",
    " - In Spark, `countByKey()` is way faster than using `groupBy()` because of less shuffling involved.\n",
    "\n",
    "Posit: Use `reduceByKey` instead of `groupByKey` if aggregation is commutative and associative. Partial aggregation reduces data movement. (Also, know the standard library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark's countByKey implementation](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs on uStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('/Users/300041370/data/sample_ustream.gz') # local\n",
    "rdd_s3 = sc.textFile('s3n://myntra-datasciences/uStream/2019/07/10/user_summary/*.gz') # s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_s3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0010a7d5.776b.4621.8517.614feff5edf0FHkLlICZ5E\\t{\"login\":\"gorurai1731@gmail.com\",\"uidx\":\"0010a7d5.776b.4621.8517.614feff5edf0FHkLlICZ5E\",\"deviceId\":\"\",\"installationId\":\"\",\"customerProfile\":{\"firstName\":\"Gurpreet\",\"lastName\":\"\",\"dob\":null,\"gender\":\"m\",\"firstLoginDate\":1474716780000,\"active\":1,\"verified\":1,\"userType\":\"regular\",\"appId\":1,\"channel\":\"email\",\"platform\":\"\",\"device\":\"\",\"profileInfoId\":\"30703094\",\"createdBy\":\"erpMessageQueue\",\"createdOn\":1474716799000,\"lastModifiedTime\":1474716799000},\"visitSummary\":[{\"visitStartTimestamp\":1474696999618,\"userAgent\":\"Mozilla/5.0 (Linux; Android 5.1.1; Redmi Note 3 Build/LMY47V; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.124 Mobile Safari/537.36; MyntraRetailAndroid/3.8.1 (Phone, 480dpi); MyntraAndroid/3.8.1 (Phone, 480dpi); api;\",\"uuid\":\"c963dd4a945c959c\",\"deviceData\":{\"appVersion\":null,\"osFamily\":null,\"osVersion\":null,\"height\":null,\"width\":null,\"deviceManufacturer\":null,\"deviceModelNo\":null,\"deviceYear\":null,\"mobileCarrier\":null,\"networkType\":null,\"bandwidth\":null,\"bandwidthBucket\":null,\"mobileDetails\":null},\"geoLocationData\":{\"city\":\"Unknown\",\"state\":\"Unknown\",\"country\":\"Unknown\",\"latitude\":\"Unknown\",\"longitude\":\"Unknown\"},\"clientIP\":\"169.149.128.244\",\"listEvents\":[],\"pdpEvents\":[],\"cartEvents\":[],\"orderEvents\":[],\"appLaunchEvents\":[]}],\"orderSummary\":[],\"itemsAddedToWishlist\":[],\"itemsRemovedFromWishlist\":[],\"wishlist\":[],\"migrated\":false}']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**uStream**: Every line is lifetime data for a user. It's a tab-separated string comprising 2 values: _uidx_ & json of lifetime activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"login\":\"gorurai1731@gmail.com\",\"uidx\":\"0010a7d5.776b.4621.8517.614feff5edf0FHkLlICZ5E\",\"deviceId\":\"\",\"installationId\":\"\",\"customerProfile\":{\"firstName\":\"Gurpreet\",\"lastName\":\"\",\"dob\":null,\"gender\":\"m\",\"firstLoginDate\":1474716780000,\"active\":1,\"verified\":1,\"userType\":\"regular\",\"appId\":1,\"channel\":\"email\",\"platform\":\"\",\"device\":\"\",\"profileInfoId\":\"30703094\",\"createdBy\":\"erpMessageQueue\",\"createdOn\":1474716799000,\"lastModifiedTime\":1474716799000},\"visitSummary\":[{\"visitStartTimestamp\":1474696999618,\"userAgent\":\"Mozilla/5.0 (Linux; Android 5.1.1; Redmi Note 3 Build/LMY47V; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.124 Mobile Safari/537.36; MyntraRetailAndroid/3.8.1 (Phone, 480dpi); MyntraAndroid/3.8.1 (Phone, 480dpi); api;\",\"uuid\":\"c963dd4a945c959c\",\"deviceData\":{\"appVersion\":null,\"osFamily\":null,\"osVersion\":null,\"height\":null,\"width\":null,\"deviceManufacturer\":null,\"deviceModelNo\":null,\"deviceYear\":null,\"mobileCarrier\":null,\"networkType\":null,\"bandwidth\":null,\"bandwidthBucket\":null,\"mobileDetails\":null},\"geoLocationData\":{\"city\":\"Unknown\",\"state\":\"Unknown\",\"country\":\"Unknown\",\"latitude\":\"Unknown\",\"longitude\":\"Unknown\"},\"clientIP\":\"169.149.128.244\",\"listEvents\":[],\"pdpEvents\":[],\"cartEvents\":[],\"orderEvents\":[],\"appLaunchEvents\":[]}],\"orderSummary\":[],\"itemsAddedToWishlist\":[],\"itemsRemovedFromWishlist\":[],\"wishlist\":[],\"migrated\":false}']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x.split('\\t')[1]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[45] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda row: tuple(row.split('\\t'))).map(lambda x: (x[0], json.loads(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[46] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda row: tuple(row.split('\\t'))).mapValues(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda row: tuple(row.split('\\t'))).mapValues(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lambda x: func(x)` is same as `func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ustream_data = rdd.map(lambda row: tuple(row.split('\\t'))).mapValues(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ustream_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_queries(ustream_dict):\n",
    "\t'''Returns the 'listPage' field'''\n",
    "\t\n",
    "\tqueries = set() # consider same queries by a user only once\n",
    "\tfor visit in ustream_dict['visitSummary']:\n",
    "\t\tfor event in visit['listEvents']:\n",
    "\t\t\tquery = event['listPage']\n",
    "\t\t\tnormalized_query = ' '.join(query.lower().split())\n",
    "\t\t\tif event['queriedByUser']:\n",
    "\t\t\t\tqueries.add(normalized_query)\n",
    "\t\n",
    "\treturn list(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0010a7d5.776b.4621.8517.614feff5edf0FHkLlICZ5E', []),\n",
       " ('001e19de.f2e9.4cfc.8803.dd4476537cfbd1GV8dNYsH', []),\n",
       " ('00269bd9.3c52.4d25.9191.86a80612a416NhSvs2i61e',\n",
       "  ['the shirt of ucb',\n",
       "   'united colors of benetton',\n",
       "   'nike flip flops',\n",
       "   'nike men flip flops',\n",
       "   'belle fille women',\n",
       "   'the shirt of ucb t shirt']),\n",
       " ('0033dda2.5e64.4514.9c4d.8189a88989f8EZA7764LMZ',\n",
       "  ['adidas shoes for men', 'checks shirts for kids', 'checks shirts for men']),\n",
       " ('005664db.0f37.4fe6.95f6.7b74a6543119S86b5iGnrU', [])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ustream_data.mapValues(get_search_queries).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " ['the shirt of ucb',\n",
       "  'united colors of benetton',\n",
       "  'nike flip flops',\n",
       "  'nike men flip flops',\n",
       "  'belle fille women',\n",
       "  'the shirt of ucb t shirt'],\n",
       " ['adidas shoes for men', 'checks shirts for kids', 'checks shirts for men'],\n",
       " []]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ustream_data.map(lambda x: x[1]).map(get_search_queries).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the shirt of ucb',\n",
       " 'united colors of benetton',\n",
       " 'nike flip flops',\n",
       " 'nike men flip flops',\n",
       " 'belle fille women',\n",
       " 'the shirt of ucb t shirt',\n",
       " 'adidas shoes for men',\n",
       " 'checks shirts for kids',\n",
       " 'checks shirts for men',\n",
       " 'nike air max infurate low basketball shoes']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ustream_data.map(lambda x: x[1]).flatMap(get_search_queries).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ustream_data.map(lambda x: x[1]).flatMap(get_search_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit <localhost:4040>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the shirt of ucb', 1),\n",
       " ('united colors of benetton', 34),\n",
       " ('nike flip flops', 38),\n",
       " ('nike men flip flops', 9),\n",
       " ('belle fille women', 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.map(lambda x: (x, 1)).groupByKey().mapValues(sum).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the shirt of ucb', 1),\n",
       " ('united colors of benetton', 34),\n",
       " ('nike flip flops', 38),\n",
       " ('nike men flip flops', 9),\n",
       " ('belle fille women', 2)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce and Spark\n",
    " - MapReduce doesn't modify input and doesn't have side effects. No of map tasks = no of input file blocks.\n",
    " - Mapper JAR is copied to all machines (**_putting computation near the data_**).\n",
    " - The process of partitioning by reducer, sorting, and copying data partitions from mappers to reducers is known as the _shuffle_ (_collate_). Hence, it's actually, *map-shuffle-reduce*.\n",
    " \n",
    "**_The problem with MapReduce_** is that for multi-job complex workflows, we need to [materialize intermediate state](https://ap-southeast-1.console.aws.amazon.com/elasticmapreduce/home?region=ap-southeast-1#cluster-details:j-VZW1Z65C0W3M), i.e., we need to write output of one job in a distributed filesystem to input it to another job.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg House Prices\n",
    "\n",
    "How to find average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices = sc.textFile('/Users/300041370/data/house-prices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = house_prices.map(lambda x: (int(x.split(',')[1]), int(x.split(',')[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 114300), (4, 114200), (3, 114800), (3, 94700), (3, 119800)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_summed = prices.mapValues(lambda price: (price, 1))\\\n",
    "                .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 115260), (3, 125732), (4, 154265), (5, 169550)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_summed.mapValues(lambda x: x[0]//x[1]).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we find _median_ house price in this fashion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame \n",
    " * A fusion of Spark RDD and Pandas DataFrame. (RDD with schema or named columns)\n",
    " * Can be read from _json_, _parquet_, _jdbc_, _orc_, _csv_\n",
    " * MLlib & Structured Streaming are based on DataFrame\n",
    " * Works best on flat datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df = spark.read.csv('/Users/300041370/data/house-prices.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Home: integer (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- SqFt: integer (nullable = true)\n",
      " |-- Bedrooms: integer (nullable = true)\n",
      " |-- Bathrooms: integer (nullable = true)\n",
      " |-- Offers: integer (nullable = true)\n",
      " |-- Brick: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+--------+---------+------+-----+------------+\n",
      "|Home| Price|SqFt|Bedrooms|Bathrooms|Offers|Brick|Neighborhood|\n",
      "+----+------+----+--------+---------+------+-----+------------+\n",
      "|   1|114300|1790|       2|        2|     2|   No|        East|\n",
      "|   2|114200|2030|       4|        2|     3|   No|        East|\n",
      "|   3|114800|1740|       3|        2|     1|   No|        East|\n",
      "|   4| 94700|1980|       3|        2|     3|   No|        East|\n",
      "|   5|119800|2130|       3|        3|     3|   No|        East|\n",
      "|   6|114600|1780|       3|        2|     2|   No|       North|\n",
      "|   7|151600|1830|       3|        3|     3|  Yes|        West|\n",
      "|   8|150700|2160|       4|        2|     2|   No|        West|\n",
      "|   9|119200|2110|       4|        2|     3|   No|        East|\n",
      "|  10|104000|1730|       3|        3|     3|   No|        East|\n",
      "+----+------+----+--------+---------+------+-----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[125700.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.approxQuantile(col='Price', probabilities=[0.5], relativeError=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Greenwald-Khanna algorithm.\n",
    "\n",
    "$ (p-\\epsilon)*N <= rank(x) <= (p+\\epsilon)*N $\n",
    "\n",
    "Spark implements other approximate algorithms too: HyperLogLog, LSH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('Price < 1000000)\n",
      "+- AnalysisBarrier\n",
      "      +- Filter (Price#11 > 100000)\n",
      "         +- Sort [Price#11 ASC NULLS FIRST], true\n",
      "            +- Relation[Home#10,Price#11,SqFt#12,Bedrooms#13,Bathrooms#14,Offers#15,Brick#16,Neighborhood#17] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Home: int, Price: int, SqFt: int, Bedrooms: int, Bathrooms: int, Offers: int, Brick: string, Neighborhood: string\n",
      "Filter (Price#11 < 1000000)\n",
      "+- Filter (Price#11 > 100000)\n",
      "   +- Sort [Price#11 ASC NULLS FIRST], true\n",
      "      +- Relation[Home#10,Price#11,SqFt#12,Bedrooms#13,Bathrooms#14,Offers#15,Brick#16,Neighborhood#17] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Price#11 ASC NULLS FIRST], true\n",
      "+- Filter ((isnotnull(Price#11) && (Price#11 > 100000)) && (Price#11 < 1000000))\n",
      "   +- Relation[Home#10,Price#11,SqFt#12,Bedrooms#13,Bathrooms#14,Offers#15,Brick#16,Neighborhood#17] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Sort [Price#11 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(Price#11 ASC NULLS FIRST, 200)\n",
      "   +- *(1) Project [Home#10, Price#11, SqFt#12, Bedrooms#13, Bathrooms#14, Offers#15, Brick#16, Neighborhood#17]\n",
      "      +- *(1) Filter ((isnotnull(Price#11) && (Price#11 > 100000)) && (Price#11 < 1000000))\n",
      "         +- *(1) FileScan csv [Home#10,Price#11,SqFt#12,Bedrooms#13,Bathrooms#14,Offers#15,Brick#16,Neighborhood#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/300041370/data/house-prices.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Price), GreaterThan(Price,100000), LessThan(Price,1000000)], ReadSchema: struct<Home:int,Price:int,SqFt:int,Bedrooms:int,Bathrooms:int,Offers:int,Brick:string,Neighborhoo...\n"
     ]
    }
   ],
   "source": [
    "house_df.orderBy('Price').filter('Price > 100000').filter('Price < 1000000').explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = house_df.groupBy('Bedrooms').avg('Price').filter(house_df['Bedrooms'] < 4).filter(house_df['Bedrooms'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Filter (Bedrooms#13 > 1)\n",
      "+- AnalysisBarrier\n",
      "      +- Filter (Bedrooms#13 < 4)\n",
      "         +- Aggregate [Bedrooms#13], [Bedrooms#13, avg(cast(Price#11 as bigint)) AS avg(Price)#73]\n",
      "            +- Relation[Home#10,Price#11,SqFt#12,Bedrooms#13,Bathrooms#14,Offers#15,Brick#16,Neighborhood#17] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Bedrooms: int, avg(Price): double\n",
      "Filter (Bedrooms#13 > 1)\n",
      "+- Filter (Bedrooms#13 < 4)\n",
      "   +- Aggregate [Bedrooms#13], [Bedrooms#13, avg(cast(Price#11 as bigint)) AS avg(Price)#73]\n",
      "      +- Relation[Home#10,Price#11,SqFt#12,Bedrooms#13,Bathrooms#14,Offers#15,Brick#16,Neighborhood#17] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [Bedrooms#13], [Bedrooms#13, avg(cast(Price#11 as bigint)) AS avg(Price)#73]\n",
      "+- Project [Price#11, Bedrooms#13]\n",
      "   +- Filter ((isnotnull(Bedrooms#13) && (Bedrooms#13 < 4)) && (Bedrooms#13 > 1))\n",
      "      +- Relation[Home#10,Price#11,SqFt#12,Bedrooms#13,Bathrooms#14,Offers#15,Brick#16,Neighborhood#17] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Bedrooms#13], functions=[avg(cast(Price#11 as bigint))], output=[Bedrooms#13, avg(Price)#73])\n",
      "+- Exchange hashpartitioning(Bedrooms#13, 200)\n",
      "   +- *(1) HashAggregate(keys=[Bedrooms#13], functions=[partial_avg(cast(Price#11 as bigint))], output=[Bedrooms#13, sum#81, count#82L])\n",
      "      +- *(1) Project [Price#11, Bedrooms#13]\n",
      "         +- *(1) Filter ((isnotnull(Bedrooms#13) && (Bedrooms#13 < 4)) && (Bedrooms#13 > 1))\n",
      "            +- *(1) FileScan csv [Price#11,Bedrooms#13] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/300041370/data/house-prices.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Bedrooms), LessThan(Bedrooms,4), GreaterThan(Bedrooms,1)], ReadSchema: struct<Price:int,Bedrooms:int>\n"
     ]
    }
   ],
   "source": [
    "query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called **_Predicate Pushdown_** and **_Column Pruning_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|Bedrooms|        avg(Price)|\n",
      "+--------+------------------+\n",
      "|       3|125732.83582089552|\n",
      "|       2|          115260.0|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD & DataFrame interconversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Home=1, Price=114300, SqFt=1790, Bedrooms=2, Bathrooms=2, Offers=2, Brick='No', Neighborhood='East'),\n",
       " Row(Home=2, Price=114200, SqFt=2030, Bedrooms=4, Bathrooms=2, Offers=3, Brick='No', Neighborhood='East'),\n",
       " Row(Home=3, Price=114800, SqFt=1740, Bedrooms=3, Bathrooms=2, Offers=1, Brick='No', Neighborhood='East')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 114300), (4, 114200)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('NumBedrooms', IntegerType()), StructField('Price', IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Home,IntegerType,true),StructField(Price,IntegerType,true),StructField(SqFt,IntegerType,true),StructField(Bedrooms,IntegerType,true),StructField(Bathrooms,IntegerType,true),StructField(Offers,IntegerType,true),StructField(Brick,StringType,true),StructField(Neighborhood,StringType,true)))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = sqlContext.createDataFrame(prices, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|NumBedrooms| Price|\n",
      "+-----------+------+\n",
      "|          2|114300|\n",
      "|          4|114200|\n",
      "|          3|114800|\n",
      "+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prices_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pi Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pi_estimation.gif](assets/pi_estimation.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def inside(p):\n",
    "    return random.random()**2 + random.random()**2 < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM = 1000000\n",
    "cnt = sc.parallelize(range(NUM)).filter(inside).count()\n",
    "cnt*4.0/NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Repartitioning_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "inside_count = sc.parallelize(range(10000000), NUM_PARTITIONS).filter(inside).count()\n",
    "round(time.time() - t1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to tune?**\n",
    " * Too few partitions – Cannot utilize all cores available in the cluster.\n",
    " * Too many partitions – Excessive overhead in managing many small tasks.\n",
    " \n",
    "1 task processes 1 partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`glom`**: From partition to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = range(int(1e6))\n",
    "ls_rdd = sc.parallelize(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_rdd.reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_rdd.glom().map(max).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glom` can be helpful for matrix multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct Names\n",
    "\n",
    "**Problem**: Find number of distinct names per _first letter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = sc.textFile('/Users/300041370/data/names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[76] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.sample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nanete',\n",
       " 'Afton',\n",
       " 'Kiele',\n",
       " 'Gabriella',\n",
       " 'Jenn',\n",
       " 'Madalena',\n",
       " 'Andriana',\n",
       " 'Sharlene',\n",
       " 'Rafa',\n",
       " 'Alayne']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.takeSample(False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788.6038429737091\n"
     ]
    }
   ],
   "source": [
    "names.map(lambda name: (name[0], name)).groupByKey().mapValues(lambda names: len(set(names))).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500.3807621002197\n"
     ]
    }
   ],
   "source": [
    "names.distinct().map(lambda name: name[0]).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find **_mutual friends_** given an adjacency list of friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = ['ABCD', 'BACDE', 'CABDE', 'DABCE', 'EBCD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_graph = [(list(f)[0], list(f)[1:]) for f in friends]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', ['B', 'C', 'D']),\n",
       " ('B', ['A', 'C', 'D', 'E']),\n",
       " ('C', ['A', 'B', 'D', 'E']),\n",
       " ('D', ['A', 'B', 'C', 'E']),\n",
       " ('E', ['B', 'C', 'D'])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_rdd = sc.parallelize(friends_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[78] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_rdd.map(lambda x: [(x[0], l) for l in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[79] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_rdd.map(lambda x: [((x[0], l), list(set(x[1]) - {l})) for l in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[80] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_rdd.flatMap(lambda x: [((x[0], l), list(set(x[1]) - {l})) for l in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[81] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_rdd.flatMap(lambda x: [(sorted((x[0], l)), list(set(x[1]) - {l})) for l in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_friends = friends_rdd.flatMap(lambda x: [(sorted((x[0], l)), list(set(x[1]) - {l})) for l in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_friends.reduceByKey(lambda x, y: set(x) & set(y)).collect() # gives error; what's the issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[unhashable type: 'list'](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_friends = friends_rdd.flatMap(lambda x: [(tuple(sorted((x[0], l))), list(set(x[1]) - {l})) for l in x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('A', 'B'), ['C', 'D']),\n",
       " (('A', 'C'), ['B', 'D']),\n",
       " (('A', 'D'), ['B', 'C']),\n",
       " (('A', 'B'), ['C', 'D', 'E']),\n",
       " (('B', 'C'), ['D', 'A', 'E']),\n",
       " (('B', 'D'), ['C', 'A', 'E']),\n",
       " (('B', 'E'), ['C', 'A', 'D']),\n",
       " (('A', 'C'), ['B', 'D', 'E']),\n",
       " (('B', 'C'), ['D', 'A', 'E']),\n",
       " (('C', 'D'), ['B', 'A', 'E']),\n",
       " (('C', 'E'), ['B', 'A', 'D']),\n",
       " (('A', 'D'), ['B', 'C', 'E']),\n",
       " (('B', 'D'), ['C', 'A', 'E']),\n",
       " (('C', 'D'), ['B', 'A', 'E']),\n",
       " (('D', 'E'), ['B', 'C', 'A']),\n",
       " (('B', 'E'), ['C', 'D']),\n",
       " (('C', 'E'), ['B', 'D']),\n",
       " (('D', 'E'), ['B', 'C'])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_friends.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('A', 'B'), {'C', 'D'}),\n",
       " (('B', 'C'), {'A', 'D', 'E'}),\n",
       " (('C', 'D'), {'A', 'B', 'E'}),\n",
       " (('C', 'E'), {'B', 'D'}),\n",
       " (('A', 'D'), {'B', 'C'}),\n",
       " (('D', 'E'), {'B', 'C'}),\n",
       " (('A', 'C'), {'B', 'D'}),\n",
       " (('B', 'D'), {'A', 'C', 'E'}),\n",
       " (('B', 'E'), {'C', 'D'})]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_friends.reduceByKey(lambda x, y: set(x) & set(y)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    " \n",
    "1. How to do a query-level analysis on ustream & ustream_non_logged_in? `union`\n",
    "2. Filter out only listEvents from ustream and create a listEvents_ustream? `saveAsTextFile`\n",
    "3. _Distributed Shared Variables_: Accumulators (MapReduce's Counter) & Broadcast variables (config)\n",
    "4. Compute intensive: Checkpoint RDD (write to an external filesystem)\n",
    "5. How to dump an RDD into a database?\n",
    "6. How to find stages from DAG?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![driver-worker.png](assets/driver-worker.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-execution.png](assets/spark-execution.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stages.png](assets/stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('/Users/300041370/data/people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, name: string]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM people WHERE age=30').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcount in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tales_df = spark.read.text('/Users/300041370/data/shakespeare_tales.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tales_words = tales_df.select(F.explode(F.split(tales_df['value'], ' ')).alias('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4960|\n",
      "| and| 3774|\n",
      "|  to| 3577|\n",
      "|  of| 2983|\n",
      "|    | 2609|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tales_words.groupBy(tales_words['word']).count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  word|count|\n",
      "+------+-----+\n",
      "| could|  225|\n",
      "|should|  249|\n",
      "| would|  431|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tales_words.where(tales_words['word'].isin(['could', 'would', 'should']))\\\n",
    "            .groupBy(tales_words['word'])\\\n",
    "            .count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`where` is an alias for `filter`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple Stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = spark.read.csv('/Users/300041370/data/apple_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+----------+--------------------+\n",
      "|      High|               Low|             Close|      Open|               Delta|\n",
      "+----------+------------------+------------------+----------+--------------------+\n",
      "|214.499996|212.38000099999996|        214.009998|213.429998|  0.5799999999999841|\n",
      "|215.589994|        213.249994|        214.379993|214.599998|-0.22000499999998624|\n",
      "|    215.23|        210.750004|        210.969995|214.379993| -3.4099980000000016|\n",
      "|212.000006|        209.050005|            210.58|    211.75| -1.1699999999999875|\n",
      "|212.000006|209.06000500000002|211.98000499999998|210.299994|   1.680010999999979|\n",
      "+----------+------------------+------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.withColumn('Delta', app['Close'] - app['Open']).select('High', 'Low', 'Close', 'Open', 'Delta').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|     Open|            Close|\n",
      "+---------+-----------------+\n",
      "|92.199997|        91.279999|\n",
      "|92.290001|        91.860001|\n",
      "|91.849998|        90.910004|\n",
      "|    91.32|90.83000200000001|\n",
      "|    90.75|        90.279999|\n",
      "+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.filter(app['Close'] < 92).select('Open', 'Close').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|     Open|            Close|\n",
      "+---------+-----------------+\n",
      "|92.199997|        91.279999|\n",
      "|92.290001|        91.860001|\n",
      "|91.849998|        90.910004|\n",
      "|    91.32|90.83000200000001|\n",
      "|    90.75|        90.279999|\n",
      "+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.filter('Close < 92').select('Open', 'Close').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:00|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.filter((app['Close'] < 200) & (app['Open'] > 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   Low|  High|\n",
      "+------+------+\n",
      "|212.38| 214.5|\n",
      "|213.25|215.59|\n",
      "|210.75|215.23|\n",
      "|209.05| 212.0|\n",
      "|209.06| 212.0|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.select(F.round(app['Low'], 2).alias('Low'), F.round(app['High'], 2).alias('High')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|               Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app.filter(app['Low'] == 197.16).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CheatSheets\n",
    "\n",
    " * [DataCamp](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)\n",
    " * [Qubole](https://www.qubole.com/resources/pyspark-cheatsheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Madlytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "madlytics = spark.read.orc('/Users/300041370/data/sample_madlytics.zlib.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "madlytics.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "madlytics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- server_ts: string (nullable = true)\n",
      " |-- app_name: string (nullable = true)\n",
      " |-- app_build: string (nullable = true)\n",
      " |-- app_version: string (nullable = true)\n",
      " |-- ab_tests: string (nullable = true)\n",
      " |-- device_build_number: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_height: string (nullable = true)\n",
      " |-- device_imei: string (nullable = true)\n",
      " |-- installation_id: string (nullable = true)\n",
      " |-- device_manufacturer: string (nullable = true)\n",
      " |-- device_model_number: string (nullable = true)\n",
      " |-- os_api_version: string (nullable = true)\n",
      " |-- bluetooth: string (nullable = true)\n",
      " |-- nfc: string (nullable = true)\n",
      " |-- geo_lat: string (nullable = true)\n",
      " |-- geo_long: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- network_carrier: string (nullable = true)\n",
      " |-- network_ip: string (nullable = true)\n",
      " |-- network_type: string (nullable = true)\n",
      " |-- session_auto_id: string (nullable = true)\n",
      " |-- session_previous_screen: string (nullable = true)\n",
      " |-- is_first_session: string (nullable = true)\n",
      " |-- landing_screen: string (nullable = true)\n",
      " |-- request_id: string (nullable = true)\n",
      " |-- server_offset: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- utm_campaign: string (nullable = true)\n",
      " |-- utm_content: string (nullable = true)\n",
      " |-- utm_medium: string (nullable = true)\n",
      " |-- utm_source: string (nullable = true)\n",
      " |-- utm_term: string (nullable = true)\n",
      " |-- referrer_url: string (nullable = true)\n",
      " |-- referrer: string (nullable = true)\n",
      " |-- session_start_time: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- uidx: string (nullable = true)\n",
      " |-- is_logged_in: string (nullable = true)\n",
      " |-- prev_customer_id: string (nullable = true)\n",
      " |-- customer_segments: string (nullable = true)\n",
      " |-- event_meta_version: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- ev_payload: string (nullable = true)\n",
      " |-- data_set_level: string (nullable = true)\n",
      " |-- clicked_entity_type: string (nullable = true)\n",
      " |-- clicked_entity_id: string (nullable = true)\n",
      " |-- clicked_widget_type: string (nullable = true)\n",
      " |-- clicked_widget_name: string (nullable = true)\n",
      " |-- client_ts: string (nullable = true)\n",
      " |-- event_payload_version: string (nullable = true)\n",
      " |-- bandwidth: string (nullable = true)\n",
      " |-- bandwidth_bucket: string (nullable = true)\n",
      " |-- action_on_entity: string (nullable = true)\n",
      " |-- device_year: string (nullable = true)\n",
      " |-- device_year_type: string (nullable = true)\n",
      " |-- react_bundle_name: string (nullable = true)\n",
      " |-- react_bundle_version: string (nullable = true)\n",
      " |-- current_screen_hash: string (nullable = true)\n",
      " |-- previous_screen_hash: string (nullable = true)\n",
      " |-- prev_screen_name: string (nullable = true)\n",
      " |-- prev_screen_type: string (nullable = true)\n",
      " |-- prev_screen_url: string (nullable = true)\n",
      " |-- prev_screen_entity_id: string (nullable = true)\n",
      " |-- prev_screen_entity_name: string (nullable = true)\n",
      " |-- prev_screen_entity_type: string (nullable = true)\n",
      " |-- screen_variant: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- widget_whom: string (nullable = true)\n",
      " |-- widget_item_whom: string (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- child_position: string (nullable = true)\n",
      " |-- widget_h_position: string (nullable = true)\n",
      " |-- widget_v_position: string (nullable = true)\n",
      " |-- widget_item0_h_position: string (nullable = true)\n",
      " |-- widget_item0_v_position: string (nullable = true)\n",
      " |-- data_set_entity_id: string (nullable = true)\n",
      " |-- notification_class: string (nullable = true)\n",
      " |-- mrp: string (nullable = true)\n",
      " |-- discounted_price: string (nullable = true)\n",
      " |-- search_text: string (nullable = true)\n",
      " |-- suggest_usage: string (nullable = true)\n",
      " |-- suggest_text: string (nullable = true)\n",
      " |-- suggest_click_depth: string (nullable = true)\n",
      " |-- widget_items_entity_name: string (nullable = true)\n",
      " |-- widget_total_view: string (nullable = true)\n",
      " |-- widget_items_total_view: string (nullable = true)\n",
      " |-- sku_id: string (nullable = true)\n",
      " |-- prev_widget_id: string (nullable = true)\n",
      " |-- prev_widget_type: string (nullable = true)\n",
      " |-- prev_widget_name: string (nullable = true)\n",
      " |-- prev_widget_v_position: string (nullable = true)\n",
      " |-- prev_widget_h_position: string (nullable = true)\n",
      " |-- prev_screen_scroll_depth: string (nullable = true)\n",
      " |-- prev_screen_load_depth: string (nullable = true)\n",
      " |-- prev_sub_widget_type: string (nullable = true)\n",
      " |-- prev_sub_widget_name: string (nullable = true)\n",
      " |-- prev_sub_widget_entity_id: string (nullable = true)\n",
      " |-- prev_sub_widget_entity_type: string (nullable = true)\n",
      " |-- prev_sub_widget_entity_name: string (nullable = true)\n",
      " |-- prev_sub_widget_v_position: string (nullable = true)\n",
      " |-- prev_sub_widget_h_position: string (nullable = true)\n",
      " |-- custom_variable_1: string (nullable = true)\n",
      " |-- custom_variable_2: string (nullable = true)\n",
      " |-- custom_variable_3: string (nullable = true)\n",
      " |-- custom_variable_4: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- forwarded_for_ip: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- widget_name: string (nullable = true)\n",
      " |-- widget_type: string (nullable = true)\n",
      " |-- publisher_tag: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- scroll_position: string (nullable = true)\n",
      " |-- screen_url: string (nullable = true)\n",
      " |-- GAID: string (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- load_date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- data_set_type: string (nullable = true)\n",
      " |-- data_set_name: string (nullable = true)\n",
      " |-- data_set_value: string (nullable = true)\n",
      " |-- os: string (nullable = true)\n",
      " |-- os_version: string (nullable = true)\n",
      " |-- device_category: string (nullable = true)\n",
      " |-- browser_name: string (nullable = true)\n",
      " |-- browser_major: string (nullable = true)\n",
      " |-- browser_minor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "madlytics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = madlytics.filter('mrp > 1000').select(['device_manufacturer', 'event_type']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|notification_class|\n",
      "+------------------+\n",
      "|         MARKETING|\n",
      "|     TRANSACTIONAL|\n",
      "|             SALES|\n",
      "|       REMARKETING|\n",
      "|                  |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "madlytics.select('notification_class').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|device_manufacturer|count|\n",
      "+-------------------+-----+\n",
      "|             Xiaomi|25931|\n",
      "|            samsung|18978|\n",
      "|              Apple|14376|\n",
      "|               vivo| 8482|\n",
      "|               OPPO| 8217|\n",
      "|            OnePlus| 7026|\n",
      "|                   | 6699|\n",
      "|           motorola| 5647|\n",
      "|             HUAWEI| 4109|\n",
      "|               asus| 1677|\n",
      "|         HMD Global| 1664|\n",
      "|             LENOVO| 1598|\n",
      "|             Realme|  567|\n",
      "|             GIONEE|  443|\n",
      "|             Google|  441|\n",
      "|               Sony|  313|\n",
      "|           Micromax|  292|\n",
      "|             lenovo|  262|\n",
      "|                LGE|  255|\n",
      "|                HTC|  249|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "madlytics.groupBy('device_manufacturer').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['count DESC NULLS LAST], true\n",
      "+- AnalysisBarrier\n",
      "      +- Aggregate [device_manufacturer#10], [device_manufacturer#10, count(1) AS count#1477L]\n",
      "         +- Relation[server_ts#0,app_name#1,app_build#2,app_version#3,ab_tests#4,device_build_number#5,device_id#6,device_height#7,device_imei#8,installation_id#9,device_manufacturer#10,device_model_number#11,os_api_version#12,bluetooth#13,nfc#14,geo_lat#15,geo_long#16,state#17,country#18,network_carrier#19,network_ip#20,network_type#21,session_auto_id#22,session_previous_screen#23,... 108 more fields] orc\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "device_manufacturer: string, count: bigint\n",
      "Sort [count#1477L DESC NULLS LAST], true\n",
      "+- Aggregate [device_manufacturer#10], [device_manufacturer#10, count(1) AS count#1477L]\n",
      "   +- Relation[server_ts#0,app_name#1,app_build#2,app_version#3,ab_tests#4,device_build_number#5,device_id#6,device_height#7,device_imei#8,installation_id#9,device_manufacturer#10,device_model_number#11,os_api_version#12,bluetooth#13,nfc#14,geo_lat#15,geo_long#16,state#17,country#18,network_carrier#19,network_ip#20,network_type#21,session_auto_id#22,session_previous_screen#23,... 108 more fields] orc\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [count#1477L DESC NULLS LAST], true\n",
      "+- Aggregate [device_manufacturer#10], [device_manufacturer#10, count(1) AS count#1477L]\n",
      "   +- Project [device_manufacturer#10]\n",
      "      +- Relation[server_ts#0,app_name#1,app_build#2,app_version#3,ab_tests#4,device_build_number#5,device_id#6,device_height#7,device_imei#8,installation_id#9,device_manufacturer#10,device_model_number#11,os_api_version#12,bluetooth#13,nfc#14,geo_lat#15,geo_long#16,state#17,country#18,network_carrier#19,network_ip#20,network_type#21,session_auto_id#22,session_previous_screen#23,... 108 more fields] orc\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [count#1477L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#1477L DESC NULLS LAST, 200)\n",
      "   +- *(2) HashAggregate(keys=[device_manufacturer#10], functions=[count(1)], output=[device_manufacturer#10, count#1477L])\n",
      "      +- Exchange hashpartitioning(device_manufacturer#10, 200)\n",
      "         +- *(1) HashAggregate(keys=[device_manufacturer#10], functions=[partial_count(1)], output=[device_manufacturer#10, count#1482L])\n",
      "            +- *(1) FileScan orc [device_manufacturer#10] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/300041370/data/sample_madlytics.zlib.orc], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<device_manufacturer:string>\n"
     ]
    }
   ],
   "source": [
    "device_counts = madlytics.groupBy('device_manufacturer').count().orderBy('count', ascending=False)\n",
    "device_counts.explain(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![query-plan-generation.png](assets/query-plan-generation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataframe-optimization.png](assets/dataframe-optimization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Logical Plan**: _dataflow_\n",
    "* **Physical Plan**: _execution flow_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          event_type|count|\n",
      "+--------------------+-----+\n",
      "|     Dismiss Filters|    6|\n",
      "|Reviews Image - R...|    1|\n",
      "|Order Confirmatio...|   10|\n",
      "|  banner-child-click|  519|\n",
      "|Cart Page - Empty...|   46|\n",
      "|Share Intent Invoked|    9|\n",
      "|JuspayTransaction...|    1|\n",
      "|           CacheSize|  791|\n",
      "|   SearchFired_brand|    3|\n",
      "|QR code - QR_scre...|    2|\n",
      "|StylePlus - FAQ W...|    6|\n",
      "|           CrossLink|   50|\n",
      "|       ProfileLogout|    6|\n",
      "|StylePlus - Histo...|    1|\n",
      "|    myOrdersTryLooks|   10|\n",
      "|  FilterOptionSelect|  360|\n",
      "|    PDPComponentLoad|43534|\n",
      "|Payment Page - pa...|   42|\n",
      "|visual-search-ini...|    4|\n",
      "|Clk_Homepage_Cart...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "madlytics.groupBy('event_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madlytics.createOrReplaceTempView('madlytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          event_type|\n",
      "+--------------------+\n",
      "|          widgetLoad|\n",
      "|broken size butto...|\n",
      "| broken size clicked|\n",
      "|          ImageSwipe|\n",
      "|       clickForOffer|\n",
      "|           ImageZoom|\n",
      "|          sizeSelect|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT event_type FROM madlytics WHERE mrp > 500\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another DataFrame Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.csv('/Users/300041370/data/sales_info.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.groupBy('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.groupBy('Company').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.groupBy('Company').agg({'Sales': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT Sales)|\n",
      "+---------------------+\n",
      "|                   11|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select(countDistinct('Sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|format_number(std, 2)|\n",
      "+---------------------+\n",
      "|               250.09|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select(stddev('Sales').alias('std')).select(format_number('std', 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark MLlib is built on DataFrames. It's a fusion of sklearn & DataFrame. Similar to sklearn, it too has Transformers, Estimators and Pipelines.\n",
    "\n",
    " * **Transformer**: Transforms one DataFrame into another, e.g. StringIndexer\n",
    " * **Estimator**: A learning algorithm, e.g. LogisticRegression.\n",
    " * **Pipeline**: Chains multiple transformers with a final estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Inspiration\n",
    " * _Computation_: MapReduce. Computation near data. Coarse-grained transformations\n",
    " * _Data Wrangling_: Pandas. DataFrame manipulation & aggregation.\n",
    " * _ML_: Scikit-learn. Transformer, Estimator, Pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-services-overview.png](assets/spark-services-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "college = spark.read.csv('/Users/300041370/data/college.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "college.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+-----------+------+---------+\n",
      "|              School|Private|Apps|Accept|Enroll|Top10perc|Top25perc|F_Undergrad|P_Undergrad|Outstate|Room_Board|Books|Personal|PhD|Terminal|S_F_Ratio|perc_alumni|Expend|Grad_Rate|\n",
      "+--------------------+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+-----------+------+---------+\n",
      "|Abilene Christian...|    Yes|1660|  1232|   721|       23|       52|       2885|        537|    7440|      3300|  450|    2200| 70|      78|     18.1|         12|  7041|       60|\n",
      "|  Adelphi University|    Yes|2186|  1924|   512|       16|       29|       2683|       1227|   12280|      6450|  750|    1500| 29|      30|     12.2|         16| 10527|       56|\n",
      "|      Adrian College|    Yes|1428|  1097|   336|       22|       50|       1036|         99|   11250|      3750|  400|    1165| 53|      66|     12.9|         30|  8735|       54|\n",
      "+--------------------+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+-----------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "college.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School',\n",
       " 'Private',\n",
       " 'Apps',\n",
       " 'Accept',\n",
       " 'Enroll',\n",
       " 'Top10perc',\n",
       " 'Top25perc',\n",
       " 'F_Undergrad',\n",
       " 'P_Undergrad',\n",
       " 'Outstate',\n",
       " 'Room_Board',\n",
       " 'Books',\n",
       " 'Personal',\n",
       " 'PhD',\n",
       " 'Terminal',\n",
       " 'S_F_Ratio',\n",
       " 'perc_alumni',\n",
       " 'Expend',\n",
       " 'Grad_Rate']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Apps', 'Accept', 'Enroll', 'Top10perc', \n",
    "                                       'Top25perc', 'F_Undergrad', 'P_Undergrad', 'Outstate', \n",
    "                                       'Room_Board', 'Books', 'Personal', 'PhD', 'Terminal', \n",
    "                                       'S_F_Ratio', 'perc_alumni', 'Expend', 'Grad_Rate'], \n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(college)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(output.columns) - set(college.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[1660.0,1232.0,72...|\n",
      "|[2186.0,1924.0,51...|\n",
      "|[1428.0,1097.0,33...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('features').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='Private', outputCol='PrivateIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60, features=DenseVector([1660.0, 1232.0, 721.0, 23.0, 52.0, 2885.0, 537.0, 7440.0, 3300.0, 450.0, 2200.0, 70.0, 78.0, 18.1, 12.0, 7041.0, 60.0]), PrivateIndex=0.0)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fixed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output_fixed.select('features', 'PrivateIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|PrivateIndex|\n",
      "+--------------------+------------+\n",
      "|[1660.0,1232.0,72...|         0.0|\n",
      "|[2186.0,1924.0,51...|         0.0|\n",
      "|[1428.0,1097.0,33...|         0.0|\n",
      "+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='PrivateIndex', featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='PrivateIndex', featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='PrivateIndex', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_preds = dtc_model.transform(test_data)\n",
    "rfc_preds = rfc_model.transform(test_data)\n",
    "gbt_preds = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_eval = BinaryClassificationEvaluator(labelCol='PrivateIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9525935374149661, 0.988095238095238, 0.98171768707483)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_eval.evaluate(dtc_preds), binary_eval.evaluate(rfc_preds), binary_eval.evaluate(gbt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "paysim = spark.read.csv('/Users/300041370/data/paysim_small.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paysim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paysim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = paysim.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='type', outputCol='typeIdx')\n",
    "va = VectorAssembler(inputCols=['typeIdx', 'amount', 'oldbalanceOrg', \n",
    "                                 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'], outputCol='features')\n",
    "dt = DecisionTreeClassifier(labelCol='isFraud', featuresCol='features', maxDepth=5)\n",
    "pipeline = Pipeline(stages=[indexer, va, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "            .addGrid(dt.maxBins, [10, 20, 30]) \\\n",
    "            .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                         estimatorParamMaps=paramGrid,\n",
    "                         evaluator=BinaryClassificationEvaluator(),\n",
    "                         numFolds=3)\n",
    "\n",
    "pipelineCV = Pipeline(stages=[indexer, va, crossval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel = pipelineCV.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pred_b = cvModel_b.transform(train_b)\n",
    "# test_pred = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryClassificationEvaluator.evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use `reduceBy` operations instead of `groupBy` if aggregation is commutative and associative.\n",
    "2. [Remember to set flags](https://spark.apache.org/docs/latest/configuration.html) like `spark.driver.maxResultSize`, `spark.executor.memory` in EMR config to get optimal performance\n",
    "3. Appropriate data format: Binary format takes less space and enforces typing. If data is columnar, _Parquet_ format is preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags and Properties\n",
    "\n",
    " * `spark.driver.maxResultSize`: when you need to `collect` huge amount of data\n",
    " * `org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator`: to allocate appropriate vcores and RAM\n",
    " * `--executor-cores`: around 5 is optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', 'localhost')\\\n",
    "        .option('port', 9999)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.select(\n",
    "            F.explode(\n",
    "                F.split(lines.value, ' ')\n",
    "            ).alias('word')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = wordCounts.writeStream.outputMode('complete').format('console').start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[EMR Console](https://ap-southeast-1.console.aws.amazon.com/elasticmapreduce/home?region=ap-southeast-1)\n",
    "\n",
    "Command to create an AWS EMR cluster,\n",
    "\n",
    "```aws emr create-cluster --name 'EMR test' --ec2-attributes KeyName=datascience-adhoc,AvailabilityZone=ap-southeast-1a --release-label emr-5.22.0 --applications Name=Spark --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.large InstanceGroupType=CORE,InstanceCount=1,InstanceType=m4.large --use-default-roles```\n",
    "\n",
    "this returns a _ClusterId_ like \"j-33BGK7M3S030C\"\n",
    "\n",
    "To ssh to this cluster,\n",
    "\n",
    "```aws emr ssh --key-pair-file ~/credentials/datascience-adhoc.pem --cluster-id j-33BGK7M3S030C```\n",
    "\n",
    "To terminate the cluster,\n",
    "\n",
    "```aws emr terminate-clusters --cluster-ids j-33BGK7M3S030C```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    " * http://shop.oreilly.com/product/9780596510046.do\n",
    " * https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf\n",
    " * http://web.stanford.edu/class/cs345d-01/rl/MRvsPDBMS.pdf\n",
    " * http://static.usenix.org/legacy/events/hotcloud10/tech/full_papers/Zaharia.pdf\n",
    " * https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\n",
    " * [Spark SQL: Relational Data Processing in Spark](http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)\n",
    " * [Discretized Streams: Fault-Tolerant Streaming Computation at Scale](http://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [_McIlroy vs Knuth_](http://www.leancrew.com/all-this/2011/12/more-shell-less-egg/) and word counts\n",
    "Jon Bentley had a regular column called “Programming Pearls” in the _Communications of the ACM_. In 1986 he got interested in literate programming, so he asked Donald Knuth to write a program in that style as a guest column and Doug McIlroy to write a literary-style critique of it. The program was\n",
    "> Read a file of text, determine the _n_ most frequently used words, and print out a sorted list of those words along with their frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knuth's 10+ pages of Pascal program used a clever, purpose-built data structure for keeping track of the words and frequency counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doug McIlroy replied with following script\n",
    "```sh\n",
    "tr -cs A-Za-z '\\n' |\n",
    "tr A-Z a-z |\n",
    "sort |\n",
    "uniq -c |\n",
    "sort -rn |\n",
    "sed ${1}q\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McIlroy, the inventor of Unix pipes, said,\n",
    "> We should have some ways of connecting programs like [a] garden hose—screw in another segment when it becomes necessary to massage data in another way. This is the way of I/O also.\n",
    "The idea of connecting programs with pipes became known as the Unix philosophy\n",
    "1. Make each program do one thing well. (Sort spills to disk and uses multiple threads)\n",
    "2. Expect the output of every program to become the input to another, as yet unknown, program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Using these niche programs, we can **_compose_** a powerful data processing job (loose coupling).\\\n",
    " - To connect any program’s output to any program’s input, all programs must use the same IO interface and data format. \n",
    " - In Unix, that interface is a file and format is ASCII (Separation of logic and wiring).\\\n",
    "\n",
    "**Pipe cons**:\n",
    " - Multiple I/O\n",
    " - Output to network connection\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
